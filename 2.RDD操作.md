[TOC]



## RDD操作

### *Spark系统介绍*

​	每个Spark应用都是由一个驱动器（drive program）来发起集群上的各种并行操作。该操作包含了相应的主函数，并定义了集群中的分布式数据集，对此数据记录了相关操作。驱动器程序通过SparkContext对象来访问Spark。然后可以创建RDD。

![ScreenClip](C:\Users\henachris\Evernote\TEMP\chrome_drag7936_3727\ScreenClip.png)



### *RDD弹性分布式数据集*

​	一个RDD是分布在不同机器上的数据的汇总，其允许某些机器上的数据出现的问题，然后通过其他机器的数据来拷贝来重现构建。包含了转化数据集的集合。

#### RDD创建操作

​	打开在本地打开spark shell，会自动生成sc.文件。sc文件的读取为sc.textFile()

```scala
//生成sc文件
scala> val lines = sc.textFile("D:\\GoogleDownloads\\spark-2.4.0-bin-hadoop2.7\\README.md")
lines: org.apache.spark.rdd.RDD[String] = D:\GoogleDownloads\spark-2.4.0-bin-hadoop2.7\README.md MapPartitionsRDD[4] at textFile at <console>:24

//选出文件中包含"Spark"的行
scala> val Sparklines = lines.filter(line => line.contains("Spark"))
Sparklines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:25

//计数
scala> lines.count
res15: Long = 105

scala> Sparklines.count()
res16: Long = 20
```

#### RDD转化操作

​	转化，用于原有的的RDD创建新的RDD，返回的是RDD！如：map函数，原始的RDD生成新的RDD.RDD之间有详细的转化数据记录，当有一个RDD坏了，可以利用记录来恢复。

![ScreenClip](C:\Users\henachris\Evernote\TEMP\chrome_drag7936_5467\ScreenClip.png)

```scala
//生成sc文件
scala> val lines = sc.parallelize(List("li lei","han mei mei"))
lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[9] at parallelize at <console>:24

//打印原始sc文件
scala> lines.foreach(println)
han mei mei
li lei

//map以空格为分隔符拆分lines
scala> val words = lines.map(line => line.split(" "))
words: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[10] at map at <console>:25

scala> words.foreach(println)
[Ljava.lang.String;@6e55205
[Ljava.lang.String;@18f5f2c

//flatMap扁平化处理，map会生成二维数组（（li,lei),(han,mei,mei))
scala> val words1 = lines.flatMap(line => line.split(" "))
words1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at flatMap at <console>:25

scala> words1.foreach(println)
li
lei
han
mei
mei
```

```scala
//parallelize生成无序的的1~50
val rdd = sc.parallelize(List(1 to 50))

//返回RDD，false为有放回抽样，每个元素被抽到的概率为0.2，抽样得到的个数不一定
rdd.sample(false,0.2)

//返回array,10个数
rdd.takeSample(false,10)

val rdd1 = sc.parallelize(List(1,2,3,4,4,4,5,5))

//distinct去重复
scala> rdd1.distinct.foreach(print)
24513

//计算笛卡尔积
rdd.cartesian(rdd1)//计算笛卡尔积

rdd.union(rdd1)//合并不去重

rdd.intersection(rdd1)//并集

rdd.subtract(rdd1)//rdd - rdd1 中的元素
```

#### RDD行动操作

​	将计算结果返回驱动器程序或者写入外部存储系统。返回值是其他类型。如，reduce汇总的函数，返回值。

​	**RDD具有惰性操作**，只有执行行动操作命令式，才会去选择最优的方法去执行之前的命令，如读文件、reduce等。不过，他会记住之前的命令。惰性求值。line.persist()缓存文件，缓存持久化。

![1552893096490](C:\Users\henachris\AppData\Roaming\Typora\typora-user-images\1552893096490.png)

```scala
scala> val rdd = sc.parallelize(1 to 5)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at parallelize at <console>:24

//reduce，累加1+2+3+4+5
scala> val sum = rdd.reduce((x,y)=>x+y)
sum: Int = 15

//fold,累加1+2+3+4+5
val sum1 = rdd.fold(0)((x,y)=>x+y)

//count 计数
val countRDD = rdd.count()

//读取第一个元素
val firstRdd = rdd.first()

//输出前五个元素
val takeRDD = rdd.take(5)

//从高到低输出前三个元素
val topRDD = rdd.top(3)

//累减-1-2-3-4-5=-15
val aggRDD = rdd.aggregate(0)((x,y)=>x-y,(x,y)=>x+y)

//第一个元素累加，第二个元素累加1
scala> val aggRDD1 = rdd.aggregate((0,0))((x,y)=>(x._1+y,x._2+1),(x1,y1)=>(x1._1+y1._1,x1._2+y1._2))
aggRDD1: (Int, Int) = (15,5)


rdd.foreach{println}
val rdd = sc.parallelize(List(1,2,3,3,4,4,4))
val counBV = rdd.countByValue
```

### *键值对操作*

![1552896034601](C:\Users\henachris\AppData\Roaming\Typora\typora-user-images\1552896034601.png)

```scala
//创建array
scala> var data = Array("Hello Spark! Welcome to be familiar with Spark,which is better then hadoop")
data: Array[String] = Array(Hello Spark! Welcome to be familiar with Spark,which is better then hadoop)

//创建RDD
scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[54] at parallelize at <console>:26

//打印
scala> rdd.foreach(print)
Hello Spark! Welcome to be familiar with Spark,which is better then hadoop

//切割RDD
scala> val rdd1 = rdd.flatMap(x=>x.split("!").flatMap(x=>x.split(",")).flatMap(x=>x.split(" ")).filter(x=>x!=""))
rdd1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at <console>:25

//map，生成键值对，键为字符，值为1。
scala> val rdd2 = rdd1.map(x=>(x,1))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[56] at map at <console>:25

//打印键值对
scala> rdd2.foreach(print)
(Hello,1)(Spark,1)(Welcome,1)(to,1)(be,1)(familiar,1)(with,1)(Spark,1)(which,1)(is,1)(better,1)(then,1)(hadoop,1)

```

```scala
//创建RDD
val lines = sc.textFile("D:\\GoogleDownloads\\spark-2.4.0-bin-hadoop2.7\\README.md")

//切割文件
val rdd1 = lines.flatMap(x=>x.split(",").flatMap(x=>x.split("")).filter(x=>x!="").filter(x=>x!="##"))

//生成了键值对
val rdd2 = rdd1.map(x=>(x,1))
```

```scala
//创建RDD文件
val rdd=sc.parallelize(Array("Hello","Spark","Welcome","Come","to","be","familiar","with","Spark","which","is","better","then","hadoop"))

//将字母转化为大写，并生成键值对
val rdd1=rdd.map(x=>x.toUpperCase).map(x=>(x,1))

//按键reduce
val rdd2=rdd1.reduceByKey((x,y)=>x+y)
scala> rdd2.foreach(print)
(TO,1)(BE,1)(COME,1)(WITH,1)(WELCOME,1)(IS,1)(SPARK,2)(BETTER,1)(FAMILIAR,1)(HELLO,1)(WHICH,1)(HADOOP,1)(THEN,1)

//按键fold
scala> rdd1.foldByKey(0)(_+_).foreach(print)
(BETTER,1)(SPARK,2)(FAMILIAR,1)(WELCOME,1)(COME,1)(WITH,1)(IS,1)(HELLO,1)(WHICH,1)(HADOOP,1)(TO,1)(THEN,1)(BE,1)

//打印键
rdd2.keys.foreach(println)

//打印值
rdd2.values.foreach(println)

//按值map
scala> rdd2.mapValues(x=>x+10).foreach(print)
(COME,11)(WELCOME,11)(IS,11)(HELLO,11)(WITH,11)(SPARK,12)(FAMILIAR,11)(WHICH,11)(HADOOP,11)(THEN,11)(BETTER,11)(TO,11)(BE,11)
```

​	**combineByKey的解释，以计算平均分为例[^1]**。

+ score => (1, score)，我们把分数作为参数,并返回了附加的元组类型。 以"XiaoMing"为列，当前其分数为88.0 =>(1,88.0)  1表示当前科目的计数器，此时只有一个科目

+ (c1: Double,Int, newScore) => (c1.\_1 + 1, c1.\_2 + newScore)，注意这里的c1就是createCombiner初始化得到的(1,88.0)。在一个分区内，我们又碰到了"XiaoMing"的一个新的分数96.0。当然我们要把之前的科目分数和当前的分数加起来即c1.\_2 + newScore,然后把科目计算器加1即c1._1 + 1

+  (c1: (Double,Int), c2:(Double,Int)) => (c1.\_1 + c2.\_1, c1.\_2 + c2.\_2)，注意"XiaoMing"可能是个学霸,他选修的科目可能过多而分散在不同的分区中。所有的分区都进行mergeValue后,接下来就是对分区间进行合并了,分区间科目数和科目数相加分数和分数相加就得到了总分和总科目数。

+ case(name,(total,count))分别接收键，总分，和计数

  ---------------------
  [^1]: 作者：PJ-Javis,来源：CSDN,原文：https://blog.csdn.net/jiangpeng59/article/details/525382

```scala
//生成RDD文件，记录三人的得分情况
scala> val Scores = Array(("XiaoMing",88.0),("LiLei",95.0),("HanMei",91.0),("LiLei",93.0),("LiLei",98.0), ("XiaoMing",96.0),("HanMei",78.0),("XiaoMing",68.0))
Scores: Array[(String, Double)] = Array((XiaoMing,88.0), (LiLei,95.0), (HanMei,91.0), (LiLei,93.0), (LiLei,98.0), (XiaoMing,96.0), (HanMei,78.0), (XiaoMing,68.0))

scala> val result = sc.parallelize(Scores)
result: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[0] at parallelize at <console>:26

//三人的计算平均分
scala> result.combineByKey(score=>(score,1),
     |     (c1:(Double,Int),newScore)=>(c1._1+newScore,c1._2+1),
     | (c1:(Double,Int),c2:(Double,Int))=>(c1._1+c2._1,c1._2+c2._2)).map{case(name,(total,count))
     |     =>(name,total/count)}.foreach(println)
(LiLei,95.33333333333333)
(XiaoMing,84.0)
(HanMei,84.5)
```



### *广播变量*

​	广播变量可以高效的向所有的工作节点发送一个较大的只读值。

```scala
val beta = (1 to 100000).map(_.toDouble).toArray[Double].map(
    x=>{if (x<11) 2.0 else 0.0})
val Beta = sc.broadcast(beta)
```



### *数值操作*

```scala
val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8))
rdd.count
rdd.mean
rdd.sum
rdd.max
rdd.variance
rdd.stdev
```

